{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06 Deep Learning (Tensorflow).ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFpdcAs71vrR",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning for Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGj0PNaJlubZ",
        "colab_type": "text"
      },
      "source": [
        "## The Dataset\n",
        "[California Housing Dataset](https://developers.google.com/machine-learning/crash-course/california-housing-data-description)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xchnxAsaKKqO",
        "colab_type": "text"
      },
      "source": [
        "## Import relevant modules\n",
        "\n",
        "The following hidden code cell imports the necessary code to run the code in the rest of this Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n9_cTveKmse",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_TaJhU4KcuY",
        "colab_type": "text"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "We use the California Housing Dataset.  The following code cell loads the separate .csv files and creates the following two pandas DataFrames:\n",
        "\n",
        "* `train_df`, which contains the training set\n",
        "* `test_df`, which contains the test set\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZlvdpyYKx7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
        "\n",
        "# shuffle the examples\n",
        "train_df = train_df.reindex(np.random.permutation(train_df.index)) \n",
        "\n",
        "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")\n",
        "\n",
        "print(train_df.shape, test_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJuguVVmMqHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu6kdR_oOLHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ldP-5z1B2vL",
        "colab_type": "text"
      },
      "source": [
        "## Normalize values\n",
        "\n",
        "When building a model with multiple features, the values of each feature should cover roughly the same range.  The following code cell normalizes datasets by converting each raw value to its [Z-score](https://en.wikipedia.org/wiki/Standard_score)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8HC-TDgB1D1",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Convert raw values to their Z-scores \n",
        "\n",
        "# Calculate the Z-scores of each column in the training set:\n",
        "train_df_mean = train_df.mean()\n",
        "train_df_std = train_df.std()\n",
        "train_df_norm = (train_df - train_df_mean)/train_df_std\n",
        "\n",
        "# Calculate the Z-scores of each column in the test set.\n",
        "# test_df_mean = test_df.mean()\n",
        "# test_df_std = test_df.std()\n",
        "test_df_norm = (test_df - train_df_mean)/train_df_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr0TQ-VzOPmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df_norm.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO1hrWlxaK55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformed_train_df = pd.concat([train_df[['longitude', 'latitude']], train_df_norm.drop(['latitude', 'longitude'], axis=1)], axis=1)\n",
        "transformed_train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzeUyKSwcfk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformed_test_df = pd.concat([test_df[['longitude', 'latitude']], test_df_norm.drop(['latitude', 'longitude'], axis=1)], axis=1)\n",
        "transformed_test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKXulrqFcz0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformed_train_df.shape, transformed_test_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9ehCgIRjTxy",
        "colab_type": "text"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "The following code cell creates a feature layer containing three features:\n",
        "\n",
        "* `latitude` X `longitude` (a feature cross)\n",
        "* `housing_median_age`\n",
        "* `total_rooms`\n",
        "* `total_bedrooms`\n",
        "* `population`\n",
        "* `households`\n",
        "* `median_income`\n",
        "\n",
        "\n",
        "This code cell specifies the features that you'll ultimately train the model on and how each of those features will be represented. The transformations (collected in `my_feature_layer`) don't actually get applied until you pass a DataFrame to it, which will happen when we train the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EkNAQhnjSu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an empty list that will eventually hold all created feature columns.\n",
        "feature_columns = []\n",
        "\n",
        "step_lat = max(transformed_train_df['latitude']) - min(transformed_train_df['latitude'])\n",
        "\n",
        "# Create a bucket feature column for latitude.\n",
        "latitude_as_a_numeric_column = tf.feature_column.numeric_column(\"latitude\")\n",
        "latitude_boundaries = list(np.arange(min(transformed_train_df['latitude']), \n",
        "                                     max(transformed_train_df['latitude']), \n",
        "                                     step_lat / 10.0))\n",
        "latitude = tf.feature_column.bucketized_column(latitude_as_a_numeric_column, \n",
        "                                               latitude_boundaries)\n",
        "\n",
        "step_lgn = max(transformed_train_df['longitude']) - min(transformed_train_df['longitude'])\n",
        "\n",
        "# Create a bucket feature column for longitude.\n",
        "longitude_as_a_numeric_column = tf.feature_column.numeric_column(\"longitude\")\n",
        "longitude_boundaries = list(np.arange(min(transformed_train_df['longitude']), \n",
        "                                      max(transformed_train_df['longitude']), \n",
        "                                      step_lgn / 10.0))\n",
        "\n",
        "longitude = tf.feature_column.bucketized_column(longitude_as_a_numeric_column, \n",
        "                                                longitude_boundaries)\n",
        "\n",
        "# Create a feature cross of latitude and longitude.\n",
        "latitude_x_longitude = tf.feature_column.crossed_column([latitude, longitude], \n",
        "                                                        hash_bucket_size=100)\n",
        "\n",
        "crossed_feature = tf.feature_column.indicator_column(latitude_x_longitude)\n",
        "feature_columns.append(crossed_feature)  \n",
        "\n",
        "# Represent housing_median_age as a floating-point value.\n",
        "housing_median_age = tf.feature_column.numeric_column(\"housing_median_age\")\n",
        "feature_columns.append(housing_median_age)\n",
        "\n",
        "# Represent total_rooms as a floating-point value.\n",
        "total_rooms = tf.feature_column.numeric_column(\"total_rooms\")\n",
        "feature_columns.append(total_rooms)\n",
        "\n",
        "# Represent total_bedrooms as a floating-point value.\n",
        "total_bedrooms = tf.feature_column.numeric_column(\"total_bedrooms\")\n",
        "feature_columns.append(total_bedrooms)\n",
        "\n",
        "# Represent population as a floating-point value.\n",
        "population = tf.feature_column.numeric_column(\"population\")\n",
        "feature_columns.append(population)\n",
        "\n",
        "# Represent households as a floating-point value.\n",
        "households = tf.feature_column.numeric_column(\"households\")\n",
        "feature_columns.append(households)\n",
        "\n",
        "# Represent median_income as a floating-point value.\n",
        "median_income = tf.feature_column.numeric_column(\"median_income\")\n",
        "feature_columns.append(median_income)\n",
        "\n",
        "# Convert the list of feature columns into a layer that will later be fed into\n",
        "# the model. \n",
        "my_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak_TMAzGOIFq",
        "colab_type": "text"
      },
      "source": [
        "## Build a linear regression model as a baseline\n",
        "\n",
        "Before creating a deep neural net, find a baseline loss by running a simple linear regression model that uses the feature layer you just created. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF0BFRXTOeR3",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Define the plotting function.\n",
        "\n",
        "def plot_the_loss_curve(mse_train, mse_val):\n",
        "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
        "\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Mean Squared Error\")\n",
        "\n",
        "  plt.plot(mse_train, label=\"Training Loss\")\n",
        "  plt.plot(mse_val, label=\"Validation Loss\")\n",
        "  plt.legend()\n",
        "  # plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
        "  plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW4Qe710LgnG",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Define functions to create and train a linear regression model\n",
        "def create_model_linear(my_learning_rate, feature_layer):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add the layer containing the feature columns to the model.\n",
        "  model.add(feature_layer)\n",
        "\n",
        "  # Add one linear layer to the model to yield a simple linear regressor.\n",
        "  model.add(tf.keras.layers.Dense(units=1))\n",
        "\n",
        "  # Construct the layers into a model that TensorFlow can execute.\n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n",
        "                loss=\"mean_squared_error\", \n",
        "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
        "  return model           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anH4A_yCcZx2",
        "colab_type": "text"
      },
      "source": [
        "## Define a training function\n",
        "\n",
        "The `train_model` function trains the model from the input features and labels. The [tf.keras.Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit) method performs the actual training. The following implementation passes a Python dictionary in which:\n",
        "\n",
        "* The *keys* are the names of each feature (for example, `longitude`, `latitude`, and so on).\n",
        "* The *value* of each key is a NumPy array containing the values of that feature. \n",
        "\n",
        "**Note:** Although you are passing *every* feature to `model.fit`, most of those values will be ignored. Only the features accessed by `my_feature_layer` will actually be used to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaAQ_rYZcVly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, dataset, epochs, batch_size, label_name):\n",
        "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
        "\n",
        "  # Split the dataset into features and label.\n",
        "  features = {name:np.array(value) for name, value in dataset.items()}\n",
        "  label = np.array(features.pop(label_name))\n",
        "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                      epochs=epochs, validation_split=0.2, shuffle=True)\n",
        "\n",
        "  # Get details that will be useful for plotting the loss curve.\n",
        "  mse_train = history.history['loss']\n",
        "  mse_val = history.history['val_loss']\n",
        "\n",
        "  return mse_train, mse_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f47LmxF5X_pu",
        "colab_type": "text"
      },
      "source": [
        "Run the following code cell to invoke the the functions defined in the preceding two code cells.\n",
        "\n",
        "**Note:** Because we've scaled all the input data, **including the label**, the resulting loss values will be smaller."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsfE4ujDL4ju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.01\n",
        "number_epochs = 20\n",
        "batch_size = 256\n",
        "label_name = \"median_house_value\"\n",
        "\n",
        "# Establish the model's topography.\n",
        "model_lr = create_model_linear(learning_rate, my_feature_layer)\n",
        "\n",
        "# Train the model on the normalized training set.\n",
        "mse_train_lr, mse_val_lr = train_model(model_lr, transformed_train_df, number_epochs, batch_size, label_name)\n",
        "plot_the_loss_curve(mse_train_lr, mse_val_lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2FNllkNc9wo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_features = {name:np.array(value) for name, value in transformed_test_df.items()}\n",
        "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
        "\n",
        "print(\"\\n Evaluate the linear regression model against the test set:\")\n",
        "result = model_lr.evaluate(x=test_features, y=test_label, batch_size=batch_size)\n",
        "\n",
        "for item in zip(model_lr.metrics_names, result):\n",
        "  print (item[0], item[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMhN5pPPZoJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_lr.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3014ezH3C7jT",
        "colab_type": "text"
      },
      "source": [
        "## Define a deep neural net model\n",
        "\n",
        "The `create_model_deep` function defines the topography of the deep neural net, specifying the following:\n",
        "\n",
        "* The number of layers in the deep neural net.\n",
        "* The number of nodes in each layer.\n",
        "\n",
        "The `create_model` function also defines the activation function of each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pedD5GhlDC-y",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def create_model_deep(my_learning_rate, my_feature_layer, layers=[20, 12]):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add the layer containing the feature columns to the model.\n",
        "  model.add(my_feature_layer)\n",
        "\n",
        "  # Describe the topography of the model by calling the tf.keras.layers.Dense\n",
        "  # method once for each layer. We've specified the following arguments:\n",
        "  #   * units specifies the number of nodes in this layer.\n",
        "  #   * activation specifies the activation function (Rectified Linear Unit).\n",
        "  #   * name is just a string that can be useful when debugging.\n",
        "\n",
        "  # Define the hidden layers\n",
        "  for index, layer in enumerate(layers):\n",
        "    model.add(tf.keras.layers.Dense(units=layer, \n",
        "                                    activation='relu', \n",
        "                                    name=f'Hidden{index}'))\n",
        "  \n",
        "  # Define the output layer.\n",
        "  model.add(tf.keras.layers.Dense(units=1, name='Output'))                              \n",
        "  \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-IXYVfvM4gD",
        "colab_type": "text"
      },
      "source": [
        "## Call the functions to build and train a deep neural net\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj3v5EKQFY8s",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Specify the label\n",
        "label_name = \"median_house_value\"\n",
        "\n",
        "# Establish the model's topography.\n",
        "model_deep = create_model_deep(learning_rate, my_feature_layer, [20, 12])\n",
        "\n",
        "# Train the model on the normalized training set. We're passing the entire\n",
        "# normalized training set, but the model will only use the features\n",
        "# defined by the feature_layer.\n",
        "\n",
        "mse_train_deep, mse_val_deep = train_model(model_deep, transformed_train_df, number_epochs, batch_size, label_name)\n",
        "plot_the_loss_curve(mse_train_deep, mse_val_deep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vlo-fWB6dAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# After building a model against the training set, test that model\n",
        "# against the test set.\n",
        "result = model_deep.evaluate(x=test_features, \n",
        "                  y=test_label, \n",
        "                  batch_size=batch_size)\n",
        "\n",
        "for item in zip(model_deep.metrics_names, result):\n",
        "  print (item[0], item[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMUmr_Xvnkok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_deep.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlPXK-SmmjQ2",
        "colab_type": "text"
      },
      "source": [
        "## Compare the two models\n",
        "\n",
        "How did the deep neural net perform against the baseline linear regression model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI7ojsL7nnBE",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "\n",
        "plt.plot(mse_train_lr, label=\"(Training Loss) Linear Model\")\n",
        "plt.plot(mse_train_deep, label=\"(Training Loss) Deep Model\")\n",
        "plt.plot(mse_val_lr, label=\"(Validation Loss) Linear Model\")\n",
        "plt.plot(mse_val_deep, label=\"(Validation Loss) Deep Model\")\n",
        "plt.legend()\n",
        "# plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
        "plt.xticks(range(21))\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5IKmk7D49_n",
        "colab_type": "text"
      },
      "source": [
        "## Optimize the deep neural network's topography\n",
        "\n",
        "Experiment with the number of layers of the deep neural network and the number of nodes in each layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYG5qXpP5a9n",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Establish the model's topography.\n",
        "model_deep_better = create_model_deep(learning_rate, my_feature_layer, [10, 8])\n",
        "\n",
        "# Train the model on the normalized training set. We're passing the entire\n",
        "# normalized training set, but the model will only use the features\n",
        "# defined by the feature_layer.\n",
        "\n",
        "mse_train_deep1, mse_val_deep1 = train_model(model_deep_better, transformed_train_df, number_epochs, batch_size, label_name)\n",
        "plot_the_loss_curve(mse_train_deep1, mse_val_deep1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX1EzDJzN2-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = model_deep_better.evaluate(x=test_features, \n",
        "                  y=test_label, \n",
        "                  batch_size=batch_size)\n",
        "\n",
        "for item in zip(model_deep.metrics_names, result):\n",
        "  print (item[0], item[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJid6C-WpRxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "\n",
        "plt.plot(mse_train_deep, label=\"(Training Loss) Deep Model\")\n",
        "plt.plot(mse_train_deep1, label=\"(Training Loss) Modified Deep Model\")\n",
        "plt.plot(mse_val_deep, label=\"(Validation Loss) Deep Model\")\n",
        "plt.plot(mse_val_deep1, label=\"(Validation Loss) Modified Deep Model\")\n",
        "plt.legend()\n",
        "# plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
        "plt.xticks(range(21))\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu7R_ZpDopIj",
        "colab_type": "text"
      },
      "source": [
        "## Regularize the deep neural network \n",
        "\n",
        "Notice that the model's loss against the test set is *higher* than the loss against the training set.  In other words, the deep neural network is *overfitting* to the data in the training set.  To reduce overfitting, regularize the model.  The course has suggested several different ways to regularize a model, including:\n",
        "\n",
        "  * *L1 regularization*\n",
        "  * *L2 regularization*\n",
        "  * *Dropout regularization*\n",
        "\n",
        "Your task is to experiment with one or more regularization mechanisms to bring the test loss closer to the training loss (while still keeping test loss relatively low).  \n",
        "\n",
        "**Note:** When you add a regularization function to a model, you might need to tweak other hyperparameters. \n",
        "\n",
        "### Implementing L1 or L2 regularization\n",
        "\n",
        "To use L1 or L2 regularization on a hidden layer, specify the `kernel_regularizer` argument to [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense). Assign one of the following methods to this argument:\n",
        "\n",
        "* `tf.keras.regularizers.l1` for L1 regularization\n",
        "* `tf.keras.regularizers.l2` for L2 regularization\n",
        "\n",
        "Each of the preceding methods takes an `l` parameter, which adjusts the *regularization rate*. Assign a decimal value between 0 and 1.0 to `l`; the higher the decimal, the greater the regularization. For example, the following applies L2 regularization at a strength of 0.05. \n",
        "\n",
        "```\n",
        "model.add(tf.keras.layers.Dense(units=20, \n",
        "                                activation='relu',\n",
        "                                kernel_regularizer=tf.keras.regularizers.l2(l=0.01),\n",
        "                                name='Hidden1'))\n",
        "```\n",
        "\n",
        "### Implementing Dropout regularization\n",
        "\n",
        "You implement dropout regularization as a separate layer in the topography. For example, the following code demonstrates how to add a dropout regularization layer between the first hidden layer and the second hidden layer:\n",
        "\n",
        "```\n",
        "model.add(tf.keras.layers.Dense( *define first hidden layer*)\n",
        " \n",
        "model.add(tf.keras.layers.Dropout(rate=0.25))\n",
        "\n",
        "model.add(tf.keras.layers.Dense( *define second hidden layer*)\n",
        "```\n",
        "\n",
        "The `rate` parameter to [tf.keras.layers.Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) specifies the fraction of nodes that the model should drop out during training. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJmtSUAlrBH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use L2 Regularization\n",
        "\n",
        "def create_model_deep_L2(my_learning_rate, my_feature_layer, layers=[20, 12], C=0.04):\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add the layer containing the feature columns to the model.\n",
        "  model.add(my_feature_layer)\n",
        "\n",
        "  # Describe the topography of the model by calling the tf.keras.layers.Dense\n",
        "  # method once for each layer. We've specified the following arguments:\n",
        "  #   * units specifies the number of nodes in this layer.\n",
        "  #   * activation specifies the activation function (Rectified Linear Unit).\n",
        "  #   * name is just a string that can be useful when debugging.\n",
        "\n",
        "  # Define the hidden layers\n",
        "  for index, layer in enumerate(layers):\n",
        "    model.add(tf.keras.layers.Dense(units=layer, \n",
        "                                    activation='relu', \n",
        "                                    kernel_regularizer=tf.keras.regularizers.l2(C),\n",
        "                                    name=f'Hidden{index}'))  \n",
        "  # Define the output layer.\n",
        "  model.add(tf.keras.layers.Dense(units=1,  \n",
        "                                  name='Output'))                              \n",
        "  \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tflt9TZEDARW",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Call the new create_model function and the other (unchanged) functions.\n",
        "\n",
        "# The following variables are the hyperparameters.\n",
        "label_name = \"median_house_value\"\n",
        "\n",
        "# Establish the model's topography.\n",
        "model_reg = create_model_deep_L2(learning_rate, my_feature_layer, [20, 10, 10, 8], C=0.001)\n",
        "\n",
        "# Train the model on the normalized training set.\n",
        "mse_train_reg, mse_val_reg = train_model(model_reg, transformed_train_df, number_epochs, batch_size, label_name)\n",
        "plot_the_loss_curve(mse_train_reg, mse_val_reg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8zh_yKBsGTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = model_reg.evaluate(x=test_features, \n",
        "                  y=test_label, \n",
        "                  batch_size=batch_size)\n",
        "\n",
        "for item in zip(model_reg.metrics_names, result):\n",
        "  print (item[0], item[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZDJMK8DG-cs",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning for Multi-Class Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVJeIw2AHMNJ",
        "colab_type": "text"
      },
      "source": [
        "## The Dataset\n",
        "  \n",
        "This MNIST dataset contains a lot of examples:\n",
        "\n",
        "* The MNIST training set contains 60,000 examples.\n",
        "* The MNIST test set contains 10,000 examples.\n",
        "\n",
        "Each example contains a pixel map showing how a person wrote a digit. For example, the following images shows how a person wrote the digit `1` and how that digit might be represented in a 14x14 pixel map (after the input data is normalized). \n",
        "\n",
        "![Two images. The first image shows a somewhat fuzzy digit one. The second image shows a 14x14 floating-point array in which most of the cells contain 0 but a few cells contain values between 0.0 and 1.0. The pattern of nonzero values corresponds to the image of the fuzzy digit in the first image.](https://www.tensorflow.org/images/MNIST-Matrix.png)\n",
        "\n",
        "Each example in the MNIST dataset consists of:\n",
        "\n",
        "* A label specified by a annotator. Each label must be an integer from 0 to 9.  For example, in the preceding image, the rater would almost certainly assign the label `1` to the example.\n",
        "* A 28x28 pixel map, where each pixel is an integer between 0 and 255. The pixel values are on a gray scale in which 0 represents white, 255 represents black, and values between 0 and 255 represent various shades of gray.  \n",
        "\n",
        "This is a multi-class classification problem with 10 output classes, one for each digit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFxjKAkrHT69",
        "colab_type": "text"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "`tf.keras` provides a set of convenience functions for loading well-known datasets. Each of these convenience functions does the following:\n",
        "\n",
        "* Loads both the training set and the test set.\n",
        "* Separates each set into features and labels.\n",
        "\n",
        "The relevant convenience function for MNIST is called `mnist.load_data()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Yd_KIaTG_IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYKlx4lmHeUL",
        "colab_type": "text"
      },
      "source": [
        "Notice that `mnist.load_data()` returned four separate values:\n",
        "\n",
        "* `x_train` contains the training set's features.\n",
        "* `y_train` contains the training set's labels.\n",
        "* `x_test` contains the test set's features.\n",
        "* `y_test` contains the test set's labels.\n",
        "\n",
        "**Note:** The MNIST .csv training set is already shuffled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJlFIVzDHhvk",
        "colab_type": "text"
      },
      "source": [
        "## View the dataset\n",
        "\n",
        "The .csv file for MNIST does not contain column names. Instead of column names, you use ordinal numbers to access different subsets of the MNIST dataset. In fact, it is probably best to think of `x_train` and `x_test` as three-dimensional NumPy arrays:  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjG1mstEHDMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Output example #2917 of the training set.\n",
        "x_train[2917]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWF8PKkdHojW",
        "colab_type": "text"
      },
      "source": [
        "Alternatively, you can call matplotlib.pyplot.imshow to interpret the preceding numeric array as an image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv68N04FHlCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use false colors to visualize the array.\n",
        "plt.imshow(x_train[2917])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bZ6463-Hz5l",
        "colab_type": "text"
      },
      "source": [
        "## Normalize feature values\n",
        "\n",
        "The following code cell maps each feature value from its current representation (an integer between 0 and 255) to a floating-point value between 0 and 1.0. Store the floating-point values in `x_train_normalized` and `x_test_normalized`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uYw7KV-Hv9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_normalized = x_train / 255.0\n",
        "x_test_normalized = x_test / 255.0\n",
        "print(x_train_normalized[2900][12]) # Output a normalized row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-02ljq0AH9Hf",
        "colab_type": "text"
      },
      "source": [
        "## Define a plotting function\n",
        "\n",
        "The following function plots an accuracy curve:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRrHuan0H3tQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_curve(hist):\n",
        "  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
        "  # list_of_metrics should be one of the names shown in:\n",
        "  # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics  \n",
        "\n",
        "  epochs = hist.epoch\n",
        "  f, ax = plt.subplots(ncols=2, figsize=(20,8))\n",
        "  ax[0].plot(epochs, hist.history['loss'], label='Training Loss')\n",
        "  ax[0].plot(epochs, hist.history['val_loss'], label='Validation Loss')\n",
        "  ax[0].set_xlabel('Epochs')\n",
        "  ax[0].set_ylabel('Loss')\n",
        "  ax[0].legend()\n",
        "  ax[1].plot(epochs, hist.history['accuracy'], label='Training Accuracy')\n",
        "  ax[1].plot(epochs, hist.history['val_accuracy'], label='Validation Accuracy')\n",
        "  ax[1].set_xlabel('Epochs')\n",
        "  ax[1].set_ylabel('Accuracy')\n",
        "  ax[1].legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H77Dp5ccISki",
        "colab_type": "text"
      },
      "source": [
        "## Create a deep neural net model\n",
        "\n",
        "The `create_model` function defines the topography of the deep neural net, specifying the following:\n",
        "\n",
        "* The number of layers in the deep neural net.\n",
        "* The number of nodes in each layer.\n",
        "* Any regularization layers.\n",
        "\n",
        "The `create_model` function also defines the *activation function* of each layer.  The activation function of the output layer is *softmax*, which will yield 10 different outputs for each example. Each of the 10 outputs provides the probability that the input example is a certain digit.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhQLRAYiICS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(my_learning_rate):\n",
        "  \"\"\"Create and compile a deep neural net.\"\"\"\n",
        "  \n",
        "  # All models in this course are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # The features are stored in a two-dimensional 28X28 array. \n",
        "  # Flatten that two-dimensional array into a a one-dimensional \n",
        "  # 784-element array.\n",
        "  model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "\n",
        "  # Define the first hidden layer.   \n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  \n",
        "  # Define a dropout regularization layer. \n",
        "  model.add(tf.keras.layers.Dropout(rate=0.2))\n",
        "\n",
        "  # Define the output layer. The units parameter is set to 10 because\n",
        "  # the model must choose among 10 possible output values (representing\n",
        "  # the digits from 0 to 9, inclusive).\n",
        "  #\n",
        "  # Don't change this layer.\n",
        "  model.add(tf.keras.layers.Dense(units=10, activation='softmax'))     \n",
        "                           \n",
        "  # Construct the layers into a model that TensorFlow can execute.  \n",
        "  # Notice that the loss function for multi-class classification\n",
        "  # is different than the loss function for binary classification.  \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
        "                loss=\"sparse_categorical_crossentropy\",\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model    \n",
        "\n",
        "\n",
        "def train_model(model, train_features, train_label, epochs,\n",
        "                batch_size=None, validation_split=0.1):\n",
        "  \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "  history = model.fit(x=train_features, y=train_label, batch_size=batch_size,\n",
        "                      epochs=epochs, shuffle=True, \n",
        "                      validation_split=validation_split)\n",
        "  \n",
        "  return history    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7IBOzcMUgW_",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_jYQktZIa47",
        "colab_type": "text"
      },
      "source": [
        "## Invoke the previous functions\n",
        "\n",
        "Run the following code cell to invoke the preceding functions and actually train the model on the training set. \n",
        "\n",
        "**Note:** Due to several factors (for example, more examples and a more complex neural network) training MNIST might take longer than training the California Housing Dataset. Be patient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxGWnejkIXxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.003\n",
        "epochs = 50\n",
        "batch_size = 4000\n",
        "validation_split = 0.2\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model = create_model(learning_rate)\n",
        "\n",
        "# Train the model on the normalized training set.\n",
        "hist = train_model(my_model, x_train_normalized, y_train, \n",
        "                           epochs, batch_size, validation_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wleA8YU_yRt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot a graph of the metric vs. epochs.\n",
        "plot_curve(hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ9TmxF0y7Fg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate against the test set.\n",
        "print(\"\\n Evaluate the new model against the test set:\")\n",
        "result = my_model.evaluate(x=x_test_normalized, y=y_test, batch_size=batch_size)\n",
        "\n",
        "for item in zip(my_model.metrics_names, result):\n",
        "  print (item[0], item[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZLHINOFIij7",
        "colab_type": "text"
      },
      "source": [
        "## Optimize the model\n",
        "\n",
        "Can we reach at least 98% accuracy against the test set? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAZfRTcaIdjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can reach 98% test accuracy with the \n",
        "# following configuration:\n",
        "#   * One hidden layer of 256 nodes; no second \n",
        "#      hidden layer.\n",
        "#   * dropout regularization rate of 0.4\n",
        "\n",
        "# We can reach 98.2% test accuracy with the \n",
        "# following configuration:\n",
        "#   * First hidden layer of 256 nodes; \n",
        "#     second hidden layer of 128 nodes.\n",
        "#   * dropout regularization rate of 0.2\n",
        "\n",
        "def create_better_model(my_learning_rate, layers=[256], dropout_rate=0.2):\n",
        "  \"\"\"Create and compile a deep neural net.\"\"\"\n",
        "  \n",
        "  # All models in this course are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # The features are stored in a two-dimensional 28X28 array. \n",
        "  # Flatten that two-dimensional array into a a one-dimensional \n",
        "  # 784-element array.\n",
        "  model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "\n",
        "  \n",
        "  for layer in layers:\n",
        "    model.add(tf.keras.layers.Dense(units=layer, activation='relu'))\n",
        "  \n",
        "    # Define a dropout regularization layer. \n",
        "    model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
        "\n",
        "  # Define the output layer. The units parameter is set to 10 because\n",
        "  # the model must choose among 10 possible output values (representing\n",
        "  # the digits from 0 to 9, inclusive).\n",
        "  #\n",
        "  # Don't change this layer.\n",
        "  model.add(tf.keras.layers.Dense(units=10, activation='softmax'))     \n",
        "                           \n",
        "  # Construct the layers into a model that TensorFlow can execute.  \n",
        "  # Notice that the loss function for multi-class classification\n",
        "  # is different than the loss function for binary classification.  \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
        "                loss=\"sparse_categorical_crossentropy\",\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2XhwPTvInCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Establish the model's topography.\n",
        "my_model_better = create_better_model(learning_rate, layers=[256, 128, 64], dropout_rate=0.2)\n",
        "\n",
        "# Train the model on the normalized training set.\n",
        "hist = train_model(my_model_better, x_train_normalized, y_train, \n",
        "                           epochs, batch_size, validation_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT_1PZSS05Bi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate against the test set.\n",
        "print(\"\\n Evaluate the new model against the test set:\")\n",
        "result = my_model_better.evaluate(x=x_test_normalized, y=y_test, batch_size=batch_size)\n",
        "\n",
        "for item in zip(my_model_better.metrics_names, result):\n",
        "  print (item[0], item[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTv_P41s0-Gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}